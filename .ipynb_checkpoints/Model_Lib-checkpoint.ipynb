{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c3b211-be52-4c2f-aa25-fe882139fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf17db-ef7b-4806-a330-22cb5533382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Layer_Perceptron(nn.Sequential):\n",
    "    def __init__(self, input_dim, intern_dim, output_dim, depth = 2, isBiased = False):\n",
    "        \n",
    "        dict = OrderedDict([(\"input\",nn.Linear(input_dim,intern_dim, bias=isBiased))])\n",
    "        for i in range(depth):\n",
    "            dict.update({str(i) : nn.Linear(intern_dim,intern_dim,bias=isBiased)})\n",
    "        dict.update({\"output\" : nn.Linear(intern_dim,output_dim,bias=isBiased)})\n",
    "\n",
    "        super().__init__(dict)\n",
    "\n",
    "        self.reset_init_weights_biases() # so that we do not use a default initialization\n",
    "\n",
    "    def reset_init_weights_biases(self, norm = None):\n",
    "        for layer in self.children():\n",
    "            if norm == None:\n",
    "                stdv = 1. / math.sqrt(layer.weight.size(1))\n",
    "            else :\n",
    "                stdv = norm\n",
    "            \n",
    "            layer.weight.data.uniform_(-stdv, stdv)\n",
    "            if layer.bias is not None:\n",
    "                layer.biases.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6264e4dd-b9a9-4771-a27c-cecc585a9b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, input_data, output_data, lossFct = nn.MSELoss(), optimizer = None, epochs = 20, init_norm = None, save = True, debug = False, savename='model.pt'):\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.SGD(model.parameters())\n",
    "    \n",
    "    if init_norm is not None:\n",
    "        model.reset_init_weights_biases(init_norm)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        y_pred = model(input_data)\n",
    "        loss = lossFct(y_pred, output_data)\n",
    "\n",
    "        if math.isnan(loss.item()):\n",
    "            print(f\"Epoch: {i+1}   Loss: {loss.item()}\")\n",
    "            break\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if debug:\n",
    "            if (i+1)%(epochs/debug) == 0:\n",
    "                print(f\"Epoch: {i+1}   Loss: {loss.item()}\")\n",
    "        \n",
    "        if save:\n",
    "            torch.save(model.state_dict(), DIRPATH+savename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9df884-154c-4465-81de-d3c96e4eb310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_data():\n",
    "        ### Data generation\n",
    "    data = np.random.multivariate_normal(\n",
    "        np.zeros(p),\n",
    "        np.ones((p,p)),\n",
    "        size=n) # shape (n,p)\n",
    "\n",
    "    w_true = np.ones(p)*1/np.sqrt(p)\n",
    "\n",
    "    observations = [np.random.normal(\n",
    "        np.dot(w_true, x),\n",
    "        sigma2)\n",
    "        for x in data]\n",
    "    observations = np.array(observations) # shape (n,)\n",
    "\n",
    "    return data, observations\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
