{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "87c3b211-be52-4c2f-aa25-fe882139fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4cf17db-ef7b-4806-a330-22cb5533382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Layer_Perceptron(nn.Sequential):\n",
    "    def __init__(self, input_dim, intern_dim, depth = 2, isBiased = False):\n",
    "        \n",
    "        dict = OrderedDict([(\"input\",nn.Linear(input_dim,intern_dim, bias=isBiased))])\n",
    "        for i in range(depth):\n",
    "            dict.update({str(i) : nn.Linear(intern_dim,intern_dim,bias=isBiased)})\n",
    "        dict.update({\"output\" : nn.Linear(intern_dim,intern_dim,bias=isBiased)})\n",
    "        \n",
    "        super().__init__(dict)\n",
    "\n",
    "    def reset_init_weights_biases(self, norm = None):\n",
    "        stdv = 0\n",
    "        for layer in self.children():\n",
    "            if norm == None:\n",
    "                stdv = 1. / math.sqrt(layer.weight.size(1))\n",
    "            else :\n",
    "                stdv = norm\n",
    "            \n",
    "            layer.weight.data.uniform_(-stdv, stdv)\n",
    "            if layer.bias is not None:\n",
    "                layer.biases.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6264e4dd-b9a9-4771-a27c-cecc585a9b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, input_data, output_data, lossFct = nn.MSELoss(), optimizer = None, epochs = 20, init_norm = None, save = True, debug = False):\n",
    "\n",
    "    if optimizer == None:\n",
    "        optimizer = torch.optim.SGD(model.parameters())\n",
    "    \n",
    "    if init_norm != None:\n",
    "        model.reset_init_weights_biases(init_norm)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        y_pred = model(input_data)\n",
    "        loss = lossFct(y_pred, output_data)\n",
    "\n",
    "        if math.isnan(loss.item()):\n",
    "            print(\"Epoch: \"+ str(i+1)+\"   Loss: \"+ str(loss.item()))\n",
    "            break\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if debug != False :\n",
    "            if (i+1)%(epochs/debug) == 0:\n",
    "                print(\"Epoch: \"+ str(i+1)+\"   Loss: \"+ str(loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa92e277-855c-47e0-beaf-2e794641a3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [0.02674451 0.03776844]\n",
      "Objective: 1.549e-01\n",
      "Solution non-linear: [0.04593633 0.0343165 ]\n",
      "Objective non-linear: 1.575e-01\n"
     ]
    }
   ],
   "source": [
    "%run main_Sam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "98a97e66-22b6-40ac-b203-9af1c978e756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([10])\n",
      "Epoch: 40000   Loss: 0.30343955755233765\n",
      "Epoch: 40000   Loss: 0.3344198167324066\n",
      "Epoch: 40000   Loss: 0.149404838681221\n",
      "Epoch: 40000   Loss: 0.003884666133671999\n",
      "Epoch: 40000   Loss: 8.127202799634858e-10\n"
     ]
    }
   ],
   "source": [
    "MLP = Multi_Layer_Perceptron(d,n, depth = 1)\n",
    "input = (torch.from_numpy(x).to(torch.float32))\n",
    "print(input.shape)\n",
    "output = (torch.from_numpy(b).to(torch.float32))\n",
    "print(output.shape)\n",
    "\n",
    "train(MLP, input, output, init_norm = None, epochs = 40000, debug = 1)\n",
    "train(MLP, input, output, init_norm = 0, epochs = 40000, debug = 1)\n",
    "train(MLP, input, output, init_norm = 1, epochs = 40000, debug = 1)\n",
    "train(MLP, input, output, init_norm = 2, epochs = 40000, debug = 1)\n",
    "train(MLP, input, output, init_norm = 10, epochs = 40000, debug = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "10aed77b-05dc-4258-a640-58f86743d61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8   Loss: nan\n",
      "Epoch: 4   Loss: nan\n",
      "Epoch: 3   Loss: nan\n"
     ]
    }
   ],
   "source": [
    "train(MLP, input, output, init_norm = 20, epochs = 40000, debug = 1)\n",
    "train(MLP, input, output, init_norm = 50, epochs = 40000, debug = 1)\n",
    "train(MLP, input, output, init_norm = 100, epochs = 40000, debug = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890aa14-1141-485e-93ca-20c8172c5d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
