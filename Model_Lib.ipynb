{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c3b211-be52-4c2f-aa25-fe882139fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4cf17db-ef7b-4806-a330-22cb5533382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP\n",
    "class MultiLayerPerceptron(nn.Sequential):\n",
    "    def __init__(self, input_dim, intern_dim, output_dim, depth = 2, isBiased = False):\n",
    "\n",
    "        self.depth = depth\n",
    "        if depth ==-1:\n",
    "            super(MultiLayerPerceptron, self).__init__()\n",
    "            self.layer = nn.Linear(input_dim, output_dim, bias=isBiased)\n",
    "        else:\n",
    "            dict = OrderedDict([(\"input\",nn.Linear(input_dim,intern_dim, bias=isBiased))])\n",
    "            for i in range(depth):\n",
    "                dict.update({str(i) : nn.Linear(intern_dim,intern_dim,bias=isBiased)})\n",
    "            dict.update({\"output\" : nn.Linear(intern_dim,output_dim,bias=isBiased)})\n",
    "            super().__init__(dict)\n",
    "\n",
    "        self.reset_init_weights_biases() # so that we do not use a default initialization\n",
    "\n",
    "    def reset_init_weights_biases(self, norm = None):\n",
    "        for layer in self.children():\n",
    "            if norm == None:\n",
    "                stdv = 1. / math.sqrt(layer.weight.size(1))\n",
    "            else :\n",
    "                stdv = norm\n",
    "            \n",
    "            layer.weight.data.uniform_(-stdv, stdv)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data.uniform_(-stdv, stdv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6264e4dd-b9a9-4771-a27c-cecc585a9b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, input_data, output_data, untilConv = -1, lossFct = nn.MSELoss(), optimizer = 'SGD', lr=0.001, epochs = 20, batch_size=None, return_vals = True, init_norm = None, save = True, debug = False, savename='model.pt'):\n",
    "\n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer == 'ASGD':\n",
    "        optimizer = torch.optim.ASGD(model.parameters(), lr=lr)\n",
    "    elif optimizer == 'GD':\n",
    "        optimizer = GD(model.parameters(), lr=lr)\n",
    "    \n",
    "    if init_norm is not None:\n",
    "        model.reset_init_weights_biases(init_norm)\n",
    "    \n",
    "    if return_vals:\n",
    "        errors = np.zeros(epochs)\n",
    "\n",
    "    n = len(input_data)\n",
    "    if batch_size is not None:\n",
    "        n_batches = n//batch_size\n",
    "\n",
    "    post_grad = 0\n",
    "    for i in range(epochs):\n",
    "        rand_idx = torch.randperm(n) # permutation of data samples\n",
    "        if batch_size is not None:\n",
    "            loss = 0\n",
    "            for t in range(n_batches):\n",
    "                idx = rand_idx[t*batch_size:(t+1)*batch_size]\n",
    "                y_pred = model(input_data[idx,:]).squeeze_()\n",
    "                loss += lossFct(y_pred, output_data[idx])\n",
    "        else:\n",
    "            y_pred = model(input_data[rand_idx,:]).squeeze_()\n",
    "            loss = lossFct(y_pred, output_data[rand_idx])\n",
    "            \n",
    "        if return_vals:\n",
    "            errors[i] = loss.item()\n",
    "\n",
    "            #if math.isnan(loss.item()):\n",
    "                #print(f\"Epoch: {i+1}   Loss: {loss.item()}\")\n",
    "                #break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        grad = 0\n",
    "        for layer in model.children():\n",
    "            grad += layer.weight.grad.mean()\n",
    "        grad = abs(grad)\n",
    "        \n",
    "        if abs(post_grad - grad) <=untilConv:\n",
    "            print(\"Convergence\")\n",
    "            break\n",
    "        post_grad = grad\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        if debug:\n",
    "            if (i+1)%(epochs/debug) == 0:\n",
    "                print(f\"Epoch: {i+1}   Loss: {loss.item():.3e}\")\n",
    "\n",
    "    if save:\n",
    "        torch.save(model.state_dict(), DIRPATH+savename)\n",
    "    \n",
    "    if return_vals:\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9df884-154c-4465-81de-d3c96e4eb310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_data(p = 100, n = 500, sigma2 = 2):\n",
    "        ### Data generation\n",
    "    data = np.random.multivariate_normal(\n",
    "        np.zeros(p),\n",
    "        np.ones((p,p)),\n",
    "        size=n) # shape (n,p)\n",
    "\n",
    "    w_true = np.ones(p)*1/np.sqrt(p)\n",
    "\n",
    "    observations = [np.random.normal(\n",
    "        np.dot(w_true, x),\n",
    "        sigma2)\n",
    "        for x in data]\n",
    "    observations = np.array(observations) # shape (n,)\n",
    "\n",
    "    return data, observations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024640f0-eca7-4019-906d-e0a8e919a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge_Lambda_Compute(A,b,LambdaArray):\n",
    "    error = np.zeros((LambdaArray.shape[0]))\n",
    "    for i in range(LambdaArray.shape[0]):\n",
    "        res = ridge(A,b,LambdaArray[i])\n",
    "        error[i] = objective(A,b,res)\n",
    "    ridgeErrorArray = np.hstack((LambdaArray[:,np.newaxis],error[:,np.newaxis]))\n",
    "    return ridgeErrorArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7234f33-fc96-4ff1-a2b2-bb915805ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weight(MLP):\n",
    "    sol_MLP = torch.eye(d)\n",
    "    for layer in MLP.children():\n",
    "        sol_MLP = sol_MLP@torch.transpose(layer.weight,0,1)\n",
    "    return sol_MLP.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63da403-5efa-4f07-a8e4-f5353514c6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
